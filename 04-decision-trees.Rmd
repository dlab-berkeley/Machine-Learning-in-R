
# Decision Trees

## Load packages

```{r}

library(rpart)
library(rpart.plot)
library(rio) # painless data import and export
library(tidyverse) # tidyverse packages 
library(tidymodels) # tidymodels framework 
library(here) # reproducible way to find files 
library(glue) # glue strings and objects 
library(patchwork) # arrange ggplots 
library(doParallel) # parallel processing 

source(here("functions", "utils.R"))

theme_set(theme_minimal())

```

## Load data 

Load `train_x_class`, `train_y_class`, `test_x_class`, and `test_y_class` variables we defined in 02-preprocessing.Rmd for this *classification* task. 

```{r}
# Objects: task_reg, task_class
load(here("data", "preprocessed.RData"))
```

## Overview

Decision trees are recursive partitioning methods that divide the predictor spaces into simpler regions and can be visualized in a tree-like structure. They attempt to classify data by dividing it into subsets according to a Y output variable and based on some predictors.  

Let's see how a decision tree classifies if a person suffers from heart disease (`target` = 1) or not (`target` = 0).

## Non-tidy

### Fit model 

```{r}

set.seed(3)

tree <- rpart::rpart(train_y_class ~ ., data = train_x_class,
             # Use method = "anova" for a continuous outcome.
             method = "class",
             
             # Can use "gini" for gini coefficient.
             parms = list(split = "information")) 

# https://stackoverflow.com/questions/4553947/decision-tree-on-information-gain

```

### Investigate 

- Here is the text-based display of the decision tree. Yikes!  :^( 

```{r}

print(tree)

```

Although interpreting the text can be intimidating, a decision tree's main strength is its tree-like plot, which is much easier to interpret.

```{r plot_tree}
rpart.plot::rpart.plot(tree) 
```

We can also look inside of `tree` to see what we can unpack. "variable.importance" is one we should check out! 

```{r}

names(tree)

tree$variable.importance

```
## Tidy models 

### parsnip 

- Build a model 

1. Specify a model 
2. Specify an engine 
3. Specify a mode 

```{r}

# workflow 
tree_wf <- workflow() %>% add_formula(target~.)

# spec 
tree_spec <- decision_tree(
  
           # Mode 
           mode = "classification",
           
           # Tuning parameters
           cost_complexity = NULL, 
           tree_depth = NULL) %>%
  set_engine("rpart") # rpart, c5.0, spark

tree_wf <- tree_wf %>% add_model(tree_spec)

```

- Fit a model

```{r}

tree_fit <- tree_wf %>% fit(train_x_class %>% bind_cols(tibble(target = train_y_class)))

```

### yardstick 

- Let's formally test prediction performance. 

**Metrics**

- `accuracy`: The proportion of the data predicted correctly 

- `precision`: Positive predictive value

- `recall` (specificity): True positive rate (e.g., healthy people really healthy)

![From wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/525px-Precisionrecall.svg.png)

- To learn more about other metrics, check out the yardstick package [references](https://yardstick.tidymodels.org/reference/index.html). 

```{r}

# Define performance metrics 

metrics <- yardstick::metric_set(accuracy, precision, recall)

# Visualize

tree_fit_viz_metr <- visualize_class_eval(tree_fit)

tree_fit_viz_metr

tree_fit_viz_mat <- visualize_class_conf(tree_fit)

tree_fit_viz_mat

```

### tune 

#### tune ingredients 

In decision trees the main hyperparameter (configuration setting) is the **complexity parameter** (CP), but the name is a little counterintuitive; a high CP results in a simple decision tree with few splits, whereas a low CP results in a larger decision tree with many splits.  

The other related hyperparameter is `tree_depth`.

```{r}

tune_spec <- 
  decision_tree(
    cost_complexity = tune(), 
    tree_depth = tune(),
    mode = "classification"
  ) %>%
  set_engine("rpart")

tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5) # 2 parameters -> 5*5 = 25 combinations 

tree_grid %>%
  count(tree_depth)

# 10-fold cross-validation

set.seed(1234) # for reproducibility 

tree_folds <- vfold_cv(train_x_class %>% bind_cols(tibble(target = train_y_class)),
                       strata = target)

```

#### Add these elements to a workflow 

```{r}

# Update workflow 
tree_wf <- tree_wf %>% update_model(tune_spec)

cl <- makeCluster(4)
registerDoParallel(cl)

# Tuning results 
tree_res <- tree_wf %>%
  tune_grid(
    resamples = tree_folds, 
    grid = tree_grid,
    metrics = metrics
  )

```

#### Visualize 

- The following plot draws on the [vignette](https://www.tidymodels.org/start/tuning/) of the tidymodels package. 

```{r}

tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  # Line + Point plot 
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  # Subplots 
  facet_wrap(~ .metric, 
             scales = "free", 
             nrow = 2) +
  # Log scale x 
  scale_x_log10(labels = scales::label_number()) +
  # Discrete color scale 
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0) +
  labs(x = "Cost complexity",
       col = "Tree depth",
       y = NULL)

```
```{r}
# Optimal parameter
best_tree <- select_best(tree_res, "recall")

best_tree

# Add the parameter to the workflow 
finalize_tree <- tree_wf %>%
  finalize_workflow(best_tree)
```

```{r}

tree_fit_tuned <- finalize_tree %>% 
  fit(train_x_class %>% bind_cols(tibble(target = train_y_class)))

# Metrics 
(tree_fit_viz_metr + labs(title = "Non-tuned")) / (visualize_class_eval(tree_fit_tuned) + labs(title = "Tuned"))

# Confusion matrix 
(tree_fit_viz_mat + labs(title = "Non-tuned")) / (visualize_class_conf(tree_fit_tuned) + labs(title = "Tuned"))

```

- Visualize variable importance 

```{r}

tree_fit_tuned %>%
  pull_workflow_fit() %>%
  vip::vip()

```
#### Test fit

- Apply the tuned model to the test dataset 

```{r}

test_fit <- finalize_tree %>% 
  fit(test_x_class %>% bind_cols(tibble(target = test_y_class)))

evaluate_class(test_fit)

```

TBD: Challenge 2 